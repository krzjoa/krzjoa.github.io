<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.99.1"><meta name=viewport content="width=device-width,initial-scale=1"><title>Time Series & torch #1 - Training a network to compute moving average &#183; krzjoa</title><meta name=description content><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png><link rel=stylesheet href=//use.fontawesome.com/releases/v5.14.0/css/all.css><link type=text/css rel=stylesheet href=/css/print.css media=print><link type=text/css rel=stylesheet href=/css/poole.css><link type=text/css rel=stylesheet href=/css/syntax.css><link type=text/css rel=stylesheet href=/css/hyde.css><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
<script type=text/x-mathjax-config>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        // https://groups.google.com/g/mathjax-users/c/FgCBLdT15nM
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {
              '.MathJax_Display': {"margin": 0},
              ".MathJax .mo, .MathJax .mi": {color: "black ! important"}
            },
            linebreaks: { automatic: true }
        }
    });
    </script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"></head><body class=theme-base-custom><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=/><h1>krzjoa</h1></a><p class=lead></p></div><nav><ul class=sidebar-nav><li><a href=/>Home</a></li><li><a href=/projects/>Projects</a></li><li><a href=/publications/>Publications</a></li></ul></nav><p><a href=https://github.com/krzjoa rel=me><i class="fab fa-github fa-lg" aria-hidden=true></i></a>
<a href=https://linkedin.com/in/krzysztof-joachimiak rel=me><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a>
<a href="https://scholar.google.com/citations?user=qNcWYHIAAAAJ%26hl" rel=me><i class="fas fa-graduation-cap fa-lg" aria-hidden=true></i></a>
<a href=https://orcid.org/0000-0003-4780-7947 rel=me><i class="fab fa-orcid fa-lg" aria-hidden=true></i></a>
<a href=/post/index.xml rel=me><i class="fas fa-rss fa-lg" aria-hidden=true></i></a></p><p>&copy; 2022. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Time Series & torch #1 - Training a network to compute moving average</h1><time datetime=2020-10-03T00:00:00Z class=post-date>Oct 3, 2020</time>
<i class="fas fa-tags"></i>
<a class="badge badge-tag" href=/tags/en>en</a>
<a class="badge badge-tag" href=/tags/r>r</a>
<a class="badge badge-tag" href=/tags/torch>torch</a>
<a class="badge badge-tag" href=/tags/ts-and-torch>ts-and-torch</a><br><br><p><a><img src=/post/2020-10-03-ts-and-torch-1/torch_ts_1.png align=center></a>
In the previous year, I published <a href=https://krzjoa.github.io/2019/12/28/pytorch-ts-v1.html>a
post</a>, which as
I hoped, was the first tutorial of the series describing how to
effectively use PyTorch in Time Series Forecasting. Recently, a new
exciting R package was submitted on CRAN. This great news was officially
announced on the <a href=https://blogs.rstudio.com/ai/posts/2020-09-29-introducing-torch-for-r/>RStudio AI Blog</a>. Yes, you mean right - the R port of
PyTorch - called simply <code>torch</code> came into play. This encouraged me to
reactivate my series, but in this time with both R and Pythonic
versions. I’ll begin with rewriting my previous-year post.</p><h3 id=1-getting-the-data>1. Getting the data</h3><p>In PyTorch version I used a Shampoo sales dataset published by Rob
Hyndman in his R package fma (a software appendix for the book
<em>Forecasting: Methods and Applications</em>). Instead of installing
Hyndman’s lib, we’ll download the dataset from the Web. It’s because
this version is already well-foramtted and we’ll avoid additional
transformation. First of all, let’s present the <code>shampoo</code> dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>library</span>(ggplot2)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(dplyr)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(data.table)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(torch)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>shampoo <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>read.csv</span>(<span style=color:#e6db74>&#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>setDT</span>(shampoo)
</span></span><span style=display:flex><span>shampoo[, n <span style=color:#f92672>:=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>.N]
</span></span></code></pre></div><h3 id=2-simple-visualization>2. Simple visualization</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>print</span>(<span style=color:#a6e22e>head</span>(shampoo))
</span></span></code></pre></div><pre><code>##    Month Sales n
## 1:  1-01 266.0 1
## 2:  1-02 145.9 2
## 3:  1-03 183.1 3
## 4:  1-04 119.3 4
## 5:  1-05 180.3 5
## 6:  1-06 168.5 6
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>ggplot</span>(shampoo) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>geom_line</span>(<span style=color:#a6e22e>aes</span>(x <span style=color:#f92672>=</span> n, y <span style=color:#f92672>=</span> Sales)) <span style=color:#f92672>+</span>
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>ggtitle</span>(<span style=color:#e6db74>&#34;Shampoo dataset&#34;</span>)
</span></span></code></pre></div><center><img src=/post/2020-10-03-ts-and-torch-1//shampoo.plot-1.png></center><p>In this plot we can see an increasing trend, but in this excercise, data
characterics make no diffeence for us.</p><h3 id=3-1-d-convolution-in-pytorch-lightning-quick-intro-or-reminder>3. 1-d convolution in PyTorch: lightning-quick intro (or reminder)</h3><p>In the case of univariate time series, one-dimensional convolution is a
sliding window applied over time series, an operation which consist of
multiplications and additions. It was intuitively illustrated on the gif
below.</p><center><img src=/post/2020-10-03-ts-and-torch-1//conv1d.gif width=400><p><strong>Source:
<a href=https://blog.floydhub.com/reading-minds-with-deep-learning/ class=uri><a href=https://blog.floydhub.com/reading-minds-with-deep-learning/>https://blog.floydhub.com/reading-minds-with-deep-learning/</a></a></strong></p></center><p>As you can see, output depend on input and kernel values. Defining
proper kernel, we can apply the operation we want. For example, using a
(0.5, 0.5) kernel, it will give us a two-element moving average. To test
that, let’s do a simple experiment.</p><h3 id=4-computing-moving-average-with-datatable>4. Computing moving average with <code>data.table</code></h3><p>Among its many features, <code>data.table</code> offers a set of ‘fast’ functions
(with names prefixed with <strong>f</strong>). One example of this great stuff is a
<a href=https://rdatatable.gitlab.io/data.table/reference/froll.html><strong><code>frollmean</code></strong></a>
functions, which computes moving average. We use a standard <code>head</code>
function as well, to limit the output. What is worth to mention is that
a <strong>NA</strong> appeared in the first row. It’s because we can’t compute moving
avearge for the first element if we haven’t added any padding on the
beginning of the array; moreover, <code>frollmean</code> keeps the input’s length,
so the first element has no value.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>ts <span style=color:#f92672>&lt;-</span> shampoo<span style=color:#f92672>$</span>Sales
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ts <span style=color:#f92672>%&gt;%</span> 
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>frollmean</span>(<span style=color:#ae81ff>2</span>) <span style=color:#f92672>%&gt;%</span> 
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>head</span>(<span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><pre><code>##  [1]     NA 205.95 164.50 151.20 149.80 174.40 200.15 228.15 208.65 157.85
</code></pre><h3 id=5-computing-moving-average-with-torch>5. Computing moving average with <code>torch</code></h3><p>Now, let’s reproduce this result using 1-dimensional convolution from
<code>torch</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>ts_tensor <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>torch_tensor</span>(ts)<span style=color:#f92672>$</span><span style=color:#a6e22e>reshape</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>-1</span>))
</span></span></code></pre></div><p>Let’s stop here for a moment. If you are not familiar with deep learning
frameworks, you would be quite confused because of this <code>reshape</code>
operation. What did we do above? We created a <strong>3-dimensional tensor</strong>;
each number in <code>reshape</code> function describes respectively:</p><ol><li>number of samples</li><li>number of channels</li><li>length of time series</li></ol><p>Meaning of this values requires some explanation.</p><ol><li><strong>Number of samples</strong> is the number of time series we are working
on. As we want to perform computations for one time series only, the
value must equal one.</li><li><strong>Number of channels</strong> is is the number of <strong>features</strong> or
(independent) <strong>variables</strong>. We don’t have any parallel variables
containing information about, say, temperature or population. It’s
clear that this value must equal one too.</li><li><strong>Length of time series</strong>. Accordingly to <code>torch</code> tensor reshaping
convention, minus one means <em>infer value for this dimension</em>. If
one-dimensional time series length has 36 elements, after reshaping
it to three-dimensional tensor with <em>number_of_samples</em> = 1 and
<em>number_of_channels</em> = 1, the last value will be equal to 36.</li></ol><p>We have to do the same with the kernel.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>kernel <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>kernel_tensor <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>torch_tensor</span>(kernel)<span style=color:#f92672>$</span><span style=color:#a6e22e>reshape</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>-1</span>))
</span></span><span style=display:flex><span><span style=color:#a6e22e>torch_conv1d</span>(ts_tensor, kernel_tensor)
</span></span></code></pre></div><pre><code>## torch_tensor 
## (1,.,.) = 
##  Columns 1 to 7  205.9500  164.5000  151.2000  149.8000  174.4000  200.1500  228.1500
## 
## Columns 8 to 14  208.6500  157.8500  229.7000  261.2000  190.1000  171.9000  179.8000
## 
## Columns 15 to 21  241.7000  232.3500  239.2000  256.5000  264.8000  296.7500  355.7500
## 
## Columns 22 to 28  343.0500  303.4000  341.0000  390.0500  378.1500  377.6000  420.3000
## 
## Columns 29 to 35  419.3500  506.4500  491.5500  544.8000  578.6500  528.3000  614.1000
## [ CPUFloatType{1,1,35} ]
</code></pre><p>As we can observe, the result is identical with values returned by
<code>frollmean</code> function. The only difference is lack of <strong>NA</strong> on the
beginning.</p><h3 id=6-learning-a-network-which-computes-moving-average>6. Learning a network, which computes moving average</h3><p>Now, let’s get to the point and train the network on the fully
controllable example. I’ve called in this manner to distinguish it from
the real-life ones. In most cases, when we train a machine learning
model, we don’t know the optimal parameter values. We are just trying to
choose the best ones, but have no guarantee that they are globally
optimal. Here, the optimal kernel value is known and should equal
<strong>[0.2, 0.2, 0.2, 0.2, 0.2]</strong>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>X_tensor <span style=color:#f92672>&lt;-</span>  <span style=color:#a6e22e>torch_tensor</span>(ts)<span style=color:#f92672>$</span><span style=color:#a6e22e>reshape</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>-1</span>))
</span></span></code></pre></div><p>In the step below, we are preparing <strong>targets</strong> (<strong>labels</strong>), which
equals to the five-element moving average.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>y <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>frollmean</span>(ts, <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>&lt;-</span> y[<span style=color:#f92672>-</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>4</span>)]
</span></span><span style=display:flex><span>y_tensor <span style=color:#f92672>&lt;-</span>  <span style=color:#a6e22e>torch_tensor</span>(y)<span style=color:#f92672>$</span><span style=color:#a6e22e>reshape</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>-1</span>))
</span></span><span style=display:flex><span>y_tensor
</span></span></code></pre></div><pre><code>## torch_tensor 
## (1,.,.) = 
##  Columns 1 to 7  178.9200  159.4200  176.6000  184.8800  199.5800  188.1000  221.7000
## 
## Columns 8 to 14  212.5200  206.4800  197.8200  215.2600  202.6200  203.7200  222.2600
## 
## Columns 15 to 21  237.5600  256.2600  259.5800  305.6200  301.1200  324.3800  331.6000
## 
## Columns 22 to 28  361.7000  340.5600  375.5200  387.3200  406.8600  433.8800  452.2200
## 
## Columns 29 to 32  500.7600  515.5600  544.3400  558.6200
## [ CPUFloatType{1,1,32} ]
</code></pre><p>We are building a one-layer convolutional neural network. It’s good to
highlight, that <strong>we don’t use any nonlinear activation function</strong>. Last
numerical value describes the length of the kernel, <em>padding = 0</em> means
that we don’t add any padding to the input, so we have to expect that
output will be “trimmed”.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>net <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>nn_conv1d</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>5</span>, padding <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, bias <span style=color:#f92672>=</span> <span style=color:#66d9ef>FALSE</span>)
</span></span></code></pre></div><p>Kernel is already initialized with, assume it for simplicity, <em>random</em>
values.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>net<span style=color:#f92672>$</span>parameters<span style=color:#f92672>$</span>weight
</span></span></code></pre></div><pre><code>## torch_tensor 
## (1,.,.) = 
##  -0.0298  0.1094 -0.4210 -0.1510 -0.1525
## [ CPUFloatType{1,1,5} ]
</code></pre><p>We can perform a convolution operation using this random value, calling
<strong>net$forward()</strong> or simply <strong>net()</strong>. This two operations are
equivalent.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>net</span>(X_tensor)
</span></span></code></pre></div><pre><code>## torch_tensor 
## (1,.,.) = 
##  Columns 1 to 7 -114.5778  -87.4777 -129.1170 -124.0212 -147.8481 -122.0550 -133.4026
## 
## Columns 8 to 14 -116.5216 -191.6899  -97.2734 -126.1265 -120.6398 -148.3641 -169.2148
## 
## Columns 15 to 21 -134.7664 -188.4784 -159.5273 -219.7331 -199.5979 -246.9963 -177.3924
## 
## Columns 22 to 28 -246.2201 -228.1574 -273.1713 -222.5049 -290.8464 -284.1429 -302.4402
## 
## Columns 29 to 32 -371.9796 -297.1908 -420.1493 -324.1110
## [ CPUFloatType{1,1,32} ]
</code></pre><p>We are initializing an optimizer object. I highly encourage you to
experiment and start with <strong>SGD</strong> which may do not converge.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># optimizer &lt;- optim_sgd(net$parameters, lr = 0.01)</span>
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>optim_adam</span>(net<span style=color:#f92672>$</span>parameters, lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>)
</span></span></code></pre></div><p>Here, he have only one example so it does not make sense to divide
training into epochs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>running_loss <span style=color:#f92672>&lt;-</span>  <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>for </span>(iteration in <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span><span style=color:#ae81ff>2000</span>) {
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#75715e># Zeroing gradients. For more,</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># see: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch</span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>$</span><span style=color:#a6e22e>zero_grad</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Forward propagation</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>&lt;-</span>  <span style=color:#a6e22e>net</span>(X_tensor)  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Mean squared error</span>
</span></span><span style=display:flex><span>    loss_value <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>torch_mean</span>((outputs <span style=color:#f92672>-</span> y_tensor)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Computing gradients</span>
</span></span><span style=display:flex><span>    loss_value<span style=color:#f92672>$</span><span style=color:#a6e22e>backward</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Changing network parameters with optimizer</span>
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>$</span><span style=color:#a6e22e>step</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Extracting loss value from tensor</span>
</span></span><span style=display:flex><span>    running_loss <span style=color:#f92672>&lt;-</span>  running_loss <span style=color:#f92672>+</span> loss_value<span style=color:#f92672>$</span><span style=color:#a6e22e>item</span>()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    flat_weights <span style=color:#f92672>&lt;-</span> net<span style=color:#f92672>$</span>parameters<span style=color:#f92672>$</span>weight <span style=color:#f92672>%&gt;%</span> 
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>as_array</span>() <span style=color:#f92672>%&gt;%</span> 
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>as.vector</span>()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>if </span>(iteration <span style=color:#f92672>%%</span> <span style=color:#ae81ff>50</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>print</span>(glue<span style=color:#f92672>::</span><span style=color:#a6e22e>glue</span>(<span style=color:#e6db74>&#34;[{iteration}] loss: {loss_value$item()}&#34;</span>))
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>print</span>(flat_weights)
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><pre><code>## [50] loss: 795.017639160156
## [1]  0.3119572  0.4480094 -0.0774434  0.1887493  0.1892590
## [100] loss: 627.464172363281
## [1]  0.30481237  0.42822435 -0.07718747  0.17363353  0.18184586
## [150] loss: 546.570983886719
## [1]  0.3097025  0.4179998 -0.0630119  0.1692921  0.1865403
## [200] loss: 471.807800292969
## [1]  0.31258762  0.40443128 -0.04937108  0.16256894  0.18939941
## [250] loss: 401.237457275391
## [1]  0.31531987  0.39036036 -0.03479132  0.15607581  0.19235790
## [300] loss: 337.717254638672
## [1]  0.31756479  0.37616777 -0.01987797  0.15002672  0.19514479
## [350] loss: 282.553039550781
## [1]  0.319161922  0.362225264 -0.005009139  0.144656733  0.197645336
## [400] loss: 235.910583496094
## [1] 0.320012957 0.348812759 0.009538475 0.140130043 0.199790746
## [450] loss: 197.225311279297
## [1] 0.32006672 0.33612481 0.02356522 0.13654210 0.20154381
## [500] loss: 165.532333374023
## [1] 0.31931198 0.32428458 0.03693568 0.13392988 0.20289351
## [550] loss: 139.712768554688
## [1] 0.31777066 0.31335631 0.04956749 0.13228267 0.20385022
## [600] loss: 118.661178588867
## [1] 0.31549129 0.30335727 0.06142059 0.13155238 0.20444071
## [650] loss: 101.386795043945
## [1] 0.31254151 0.29426861 0.07248778 0.13166353 0.20470326
## [700] loss: 87.0595397949219
## [1] 0.30900255 0.28604546 0.08278601 0.13252223 0.20468384
## [750] loss: 75.020133972168
## [1] 0.30496314 0.27862594 0.09234858 0.13402404 0.20443186
## [800] loss: 64.7659072875977
## [1] 0.3005151 0.2719381 0.1012190 0.1360608 0.2039973
## [850] loss: 55.9260444641113
## [1] 0.2957492 0.2659062 0.1094460 0.1385261 0.2034285
## [900] loss: 48.2335586547852
## [1] 0.2907525 0.2604553 0.1170791 0.1413187 0.2027697
## [950] loss: 41.4970893859863
## [1] 0.2856061 0.2555139 0.1241664 0.1443462 0.2020606
## [1000] loss: 35.5792236328125
## [1] 0.2803833 0.2510171 0.1307523 0.1475262 0.2013350
## [1050] loss: 30.3781261444092
## [1] 0.2751493 0.2469072 0.1368768 0.1507875 0.2006208
## [1100] loss: 25.8145942687988
## [1] 0.2699609 0.2431345 0.1425748 0.1540700 0.1999404
## [1150] loss: 21.8240375518799
## [1] 0.2648661 0.2396567 0.1478763 0.1573242 0.1993102
## [1200] loss: 18.3501605987549
## [1] 0.2599051 0.2364388 0.1528070 0.1605106 0.1987420
## [1250] loss: 15.3419895172119
## [1] 0.2551105 0.2334520 0.1573887 0.1635987 0.1982433
## [1300] loss: 12.7523593902588
## [1] 0.2505079 0.2306734 0.1616401 0.1665655 0.1978179
## [1350] loss: 10.5367918014526
## [1] 0.2461172 0.2280841 0.1655775 0.1693947 0.1974661
## [1400] loss: 8.65341949462891
## [1] 0.2419526 0.2256693 0.1692155 0.1720755 0.1971868
## [1450] loss: 7.06301403045654
## [1] 0.2380237 0.2234169 0.1725675 0.1746014 0.1969763
## [1500] loss: 5.72896862030029
## [1] 0.2343363 0.2213169 0.1756462 0.1769695 0.1968299
## [1550] loss: 4.61755132675171
## [1] 0.2308923 0.2193609 0.1784641 0.1791797 0.1967420
## [1600] loss: 3.69792985916138
## [1] 0.2276909 0.2175417 0.1810337 0.1812342 0.1967065
## [1650] loss: 2.94231581687927
## [1] 0.2247288 0.2158528 0.1833675 0.1831365 0.1967170
## [1700] loss: 2.32577872276306
## [1] 0.2220005 0.2142882 0.1854781 0.1848916 0.1967671
## [1750] loss: 1.82624590396881
## [1] 0.2194988 0.2128422 0.1873784 0.1865052 0.1968507
## [1800] loss: 1.42442286014557
## [1] 0.2172151 0.2115093 0.1890816 0.1879836 0.1969618
## [1850] loss: 1.10348606109619
## [1] 0.2151396 0.2102839 0.1906009 0.1893335 0.1970950
## [1900] loss: 0.849016129970551
## [1] 0.2132619 0.2091608 0.1919495 0.1905621 0.1972449
## [1950] loss: 0.648723244667053
## [1] 0.2115705 0.2081344 0.1931406 0.1916765 0.1974071
## [2000] loss: 0.492226451635361
## [1] 0.2100540 0.2071995 0.1941869 0.1926837 0.1975773
</code></pre><p>As we can see in this example, algorithm converges and parameter values
are becoming close to the <strong>true solution</strong>, i.e. <strong>[0.2, 0.2, 0.2,
0.2, 0.2]</strong>.</p><p>On my blog, you can also find a <a href=https://krzjoa.github.io/2019/12/28/pytorch-ts-v1.html>Python
version</a> of this
post.</p></div></main></body></html>